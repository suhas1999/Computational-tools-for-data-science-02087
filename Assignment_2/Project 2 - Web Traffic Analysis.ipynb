{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Web Traffic Analysis\n",
    "**This is the second of three mandatory projects to be handed in as part of the assessment for the course 02807 Computational Tools for Data Science at Technical University of Denmark, autumn 2019.**\n",
    "\n",
    "#### Practical info\n",
    "- **The project is to be done in groups of at most 3 students**\n",
    "- **Each group has to hand in _one_ Jupyter notebook (this notebook) with their solution**\n",
    "- **The hand-in of the notebook is due 2019-11-10, 23:59 on DTU Inside**\n",
    "\n",
    "#### Your solution\n",
    "- **Your solution should be in Python**\n",
    "- **For each question you may use as many cells for your solution as you like**\n",
    "- **You should document your solution and explain the choices you've made (for example by using multiple cells and use Markdown to assist the reader of the notebook)**\n",
    "- **You should not remove the problem statements**\n",
    "- **Your notebook should be runnable, i.e., clicking [>>] in Jupyter should generate the result that you want to be assessed**\n",
    "- **You are not expected to use machine learning to solve any of the exercises**\n",
    "- **You will be assessed according to correctness and readability of your code, choice of solution, choice of tools and libraries, and documentation of your solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this project your task is to analyze a stream of log entries. A log entry consists of an [IP address](https://en.wikipedia.org/wiki/IP_address) and a [domain name](https://en.wikipedia.org/wiki/Domain_name). For example, a log line may look as follows:\n",
    "\n",
    "`192.168.0.1 somedomain.dk`\n",
    "\n",
    "One log line is the result of the event that the domain name was visited by someone having the corresponding IP address. Your task is to analyze the traffic on a number of domains. Counting the number of unique IPs seen on a domain doesn't correspond to the exact number of unique visitors, but it is a good estimate.\n",
    "\n",
    "Specifically, you should answer the following questions from the stream of log entries.\n",
    "\n",
    "- How many unique IPs are there in the stream?\n",
    "- How many unique IPs are there for each domain?\n",
    "- How many times was IP X seen on domain Y? (for some X and Y provided at run time)\n",
    "\n",
    "**The answers to these questions can be approximate!**\n",
    "\n",
    "You should also try to answer one or more of the following, more advanced, questions. The answers to these should also be approximate.\n",
    "\n",
    "- How many unique IPs are there for the domains $d_1, d_2, \\ldots$?\n",
    "- How many times was IP X seen on domains $d_1, d_2, \\ldots$?\n",
    "- What are the X most frequent IPs in the stream?\n",
    "\n",
    "You should use algorithms and data structures that you've learned about in the lectures, and you should provide your own implementations of these.\n",
    "\n",
    "Furthermore, you are expected to:\n",
    "\n",
    "- Document the accuracy of your answers when using algorithms that give approximate answers\n",
    "- Argue why you are using certain parameters for your data structures\n",
    "\n",
    "This notebook is in three parts. In the first part you are given an example of how to read from the stream (which for the purpose of this project is a remote file). In the second part you should implement the algorithms and data structures that you intend to use, and in the last part you should use these for analyzing the stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the stream\n",
    "The following code reads a remote file line by line. It is wrapped in a generator to make it easier to extend. You may modify this if you want to, but your solution should remain parametrized, so that your notebook can be run without having to consume the entire file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "import mmh3\n",
    "import math\n",
    "def stream(n):\n",
    "    i = 0\n",
    "    with urllib.request.urlopen('https://files.dtu.dk/fss/public/link/public/stream/read/traffic?linkToken=3pLwj8eS8I_MkvCK&itemName=traffic') as f:\n",
    "        for line in f:\n",
    "            element = line.rstrip().decode(\"utf-8\")\n",
    "            yield element\n",
    "            i += 1\n",
    "            if i == n:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STREAM_SIZE = 1000\n",
    "web_traffic_stream = stream(STREAM_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many unique IPs are there in the stream?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question, we can use a hyperloglog structure. This enables us to estimate the number of unique ip's using very little memory. The hyper-parameter, b, is chosen to reflect a certain wanted error rate given by approximately $\\frac{1.04}{\\sqrt{2^b}}$ using b = 10, thus gives fairly good results as can be seen.\n",
    "\n",
    " Alpha is chosen by using the recommendations in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cardinality(bucket,b,alpha):\n",
    "    sum_ = 0\n",
    "    j = 0 \n",
    "    for elements in bucket:\n",
    "        if(elements == 0):\n",
    "            j+=1\n",
    "        sum_ += 2**(-1*elements)\n",
    "        #else:\n",
    "\n",
    "    card = (sum_**(-1))*((2**b)**2)*alpha\n",
    "    if card > 2**32/30:\n",
    "        #print(\"large range correction\")\n",
    "        #print(-2**32*np.log(1-card/(2**32)))\n",
    "        return -2**32 * (np.log(1-card/(2**32)))\n",
    "    elif card < 5/2 * 2 ** 32:\n",
    "        if j==0:\n",
    "            return card\n",
    "        else:\n",
    "            #print(\"{} buckets are zero\".format(j) )\n",
    "            return (2**b) * np.log(2**b/j)\n",
    "    else:\n",
    "        return card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FirstOnePos(n): \n",
    "    count = 0\n",
    "    for i in range(len(n)):\n",
    "        if n[i] == '1':\n",
    "            break;\n",
    "    return i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperlog_bucket(n,b):\n",
    "    web_traffic_stream = stream(n)\n",
    "    bucket_count = np.zeros(2**b)\n",
    "    bucket = np.zeros(2**b)\n",
    "    for i,l in enumerate(web_traffic_stream):\n",
    "        #if i%2 ==0 : print(i)\n",
    "        q = format(mmh3.hash(l.split(\"\\t\",1)[0],signed=False),'032b')\n",
    "        q_b = q[0:b]\n",
    "        m = FirstOnePos(q[b:])\n",
    "        bucket[int(q_b,2)] = max(bucket[int(q_b,2)],m)\n",
    "    return bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many times was IP X seen on domain Y? (for some X and Y provided at run time)\n",
    "In order to find how many times an IP address was seen on a domain we used the CountMin Sketch data-structure. We used murmur3 to hash the IP addredd and domain name combinations. We tried out different values for the number of hash algorithms used (height of the matrix)(d) and for the width of the matrix(w). With the values d = 10 and w = 8000 we achieved good approximation.\n",
    "\n",
    "In the implementation first we set the values for d and w. After this we intialize the matrix(d x w) with zeros.\n",
    "\n",
    "The function element_arrives(element) increases the counters based in the positions based on the hashed values. For hashing we used the mmh and in order to apply different hash algorithms from the same family we were changing the seed values.\n",
    "\n",
    "The function get_element_count(element) finds the minimum corresponding counter.\n",
    "\n",
    "By calling the countIpOnDomain(x, y), where x is the IP address and y is the domain name, we can get an estimation for the count of the IP address-domain name combination in the stream.\n",
    "\n",
    "In order to test the algorithm we read in a stream from file where we added multiple rows with the same ip-domain combination (40156 rows). The algorithm overestimated whatever the checked ip/domain was. When we tested the algorithm for an IP accured 156 times in the stream we got an estimated accurance 167, and when we tested for elements which accured 1 times we got 3,4,5 as accurance.\n",
    "\n",
    "Because of collisions the result depends heavily on the parameters d and w we select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_min_sketch\n",
    "import mmh3\n",
    "d = 10\n",
    "w = 8000\n",
    "\n",
    "M = [[0 for x in range(w)] for y in range(d)]\n",
    "\n",
    "\n",
    "def element_arrives(element):\n",
    "    for i in range(0, d):\n",
    "        M[i][mmh3.hash(element, seed=i) % w] = M[i][mmh3.hash(element, seed=i) % w] + 1\n",
    "\n",
    "def get_element_count(element):\n",
    "    min = M[0][mmh3.hash(element, seed=0) % w]\n",
    "    for i in range(1, d):\n",
    "        actual_element = M[i][mmh3.hash(element, seed=i) % w]\n",
    "        if min > actual_element:\n",
    "            min = actual_element\n",
    "    return min\n",
    "\n",
    "def countIpOnDomain(x, y):\n",
    "    return get_element_count(x + \"\\t\" + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function generateCountMinSketch(n) we fetch n element from the stream and for every element we call the element_arrives() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateCountMinSketch(n):\n",
    "    d = 10\n",
    "    w = 8000\n",
    "\n",
    "    M = [[0 for x in range(w)] for y in range(d)]\n",
    "    i = 0\n",
    "    with urllib.request.urlopen('https://files.dtu.dk/fss/public/link/public/stream/read/traffic?linkToken=3pLwj8eS8I_MkvCK&itemName=traffic') as f:\n",
    "        for line in f:\n",
    "            element = line.rstrip().decode(\"utf-8\")\n",
    "            element_arrives(element)\n",
    "            i += 1\n",
    "            if i == n:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateCountMinSketch(40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countIpOnDomain(\"124.81.124.112\",\"wikipedia.org\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many unique IPs are there for each domain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_intializer(bucket_dictionary,domain,b):\n",
    "    bucket_dictionary[domain] = np.zeros(2**b)\n",
    "def domain_unique_ip(bucket_dictionary,web_traffic_stream,b):\n",
    "    for i,l in enumerate(web_traffic_stream):\n",
    "        if l.split()[1] in bucket_dictionary.keys():\n",
    "            #print(\"c\")\n",
    "            a =0\n",
    "        else:\n",
    "            #print(\"e\")\n",
    "            bucket_intializer(bucket_dictionary,l.split()[1],b)\n",
    "        #print(\"a\")\n",
    "        q = format(mmh3.hash(l.split(\"\\t\",1)[0],signed=False),'032b')\n",
    "        #print(q)\n",
    "        q_b = q[0:b]\n",
    "        m = FirstOnePos(q[b:])\n",
    "        bucket_dictionary[l.split()[1]][int(q_b,2)] = max(bucket_dictionary[l.split()[1]][int(q_b,2)],m)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the most X frequent IP's in the stream?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our idea was to use a count-min sketch together with a heap that tracks the heavy hitter IP's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 10\n",
    "w = 20000\n",
    "\n",
    "M = [[0 for x in range(w)] for y in range(d)]\n",
    "\n",
    "def element_arrives(element):\n",
    "    for i in range(0, d):\n",
    "        M[i][mmh3.hash(element, seed=i) % w] = M[i][mmh3.hash(element, seed=i) % w] + 1\n",
    "\n",
    "def get_element_count(element):\n",
    "    min = M[0][mmh3.hash(element, seed=0) % w]\n",
    "    for i in range(1, d):\n",
    "        actual_element = M[i][mmh3.hash(element, seed=i) % w]\n",
    "        if min > actual_element:\n",
    "            min = actual_element\n",
    "    return min\n",
    "\n",
    "def generateCountMinSketch_heap(n,heap_len):\n",
    "    heap = {}\n",
    "    i = 0\n",
    "    with urllib.request.urlopen('https://files.dtu.dk/fss/public/link/public/stream/read/traffic?linkToken=3pLwj8eS8I_MkvCK&itemName=traffic') as f:\n",
    "        for line in f:\n",
    "            element = line.rstrip().decode(\"utf-8\").split(\"\\t\",1)[0]\n",
    "            element_arrives(element)\n",
    "            i += 1\n",
    "            element_count = get_element_count(element)\n",
    "            if element not in heap:\n",
    "                if len(heap) < heap_len:\n",
    "                    heap[element] = element_count\n",
    "            for j in heap:\n",
    "                heap[j] = get_element_count(j)\n",
    "            min_key = min(heap, key=heap.get)\n",
    "            if heap[min_key] < element_count:\n",
    "                heap[element] = heap.pop(min_key)\n",
    "                heap[element] = element_count\n",
    "                \n",
    "            if i == n:\n",
    "                break\n",
    "        return heap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperloglog for unique ip's in stream\n",
    "Let us compare the actual unique ip's to the estimate by HLL.\n",
    "\n",
    "For this question, we can use a hyperloglog structure. This enables us to estimate the number of unique ip's using very little memory. The hyper-parameter, b, is chosen to reflect a certain wanted error rate given by approximately $\\frac{1.04}{\\sqrt{2^b}}$ using b = 10, thus gives fairly good results as can be seen.\n",
    "\n",
    " Alpha is chosen by using the recommendations in the paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique ip's(Reference): 50000\n",
      "Number of estimated unique ip's(HLL)48603.14510159575\n",
      "Total error: 2.7937097968085%\n"
     ]
    }
   ],
   "source": [
    "# Reference\n",
    "my_dict = {}\n",
    "i = 0\n",
    "n = 50000\n",
    "with urllib.request.urlopen('https://files.dtu.dk/fss/public/link/public/stream/read/traffic?linkToken=3pLwj8eS8I_MkvCK&itemName=traffic') as f:\n",
    "    for line in f:\n",
    "        element = line.rstrip().decode(\"utf-8\").split(\"\\t\",1)[0]\n",
    "        i += 1\n",
    "        if element not in my_dict:\n",
    "                my_dict[element] = 1\n",
    "        else:\n",
    "                my_dict[element] += 1                \n",
    "        if i == n:\n",
    "             break\n",
    "b = 10\n",
    "bucket = get_hyperlog_bucket(5*10**4,b)\n",
    "exact = sum(my_dict.values())\n",
    "approx = get_cardinality(bucket,b,0.7213/(1+1.079/(2**b)))\n",
    "print(\"Number of unique ip's(Reference): \" + str(exact))\n",
    "print(\"Number of estimated unique ip's(HLL)\" + str(approx))\n",
    "print(\"Total error: \" + str((100*abs(approx-exact))/exact)+\"%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many times was IP X seen on domain Y? (for some X and Y provided at run time)\n",
    "Below we can see some tests on the stream. The first one answers how many times ip 124.81.124.112 was seen on wikipedia.org, the second one answers how many times 192.118.123.80 was seen on \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimeted no of ip:192.118.123.80 in domain:python.org is 41\n"
     ]
    }
   ],
   "source": [
    "STREAM_SIZE = 1000000\n",
    "generateCountMinSketch(STREAM_SIZE)\n",
    "ip = \"192.118.123.80\"\n",
    "domain = \"python.org\"\n",
    "estimated_number = countIpOnDomain(ip,domain)\n",
    "print (\"estimeted no of ip:{} in domain:{} is {}\".format(ip,domain,estimated_number))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 500000\n",
    "generateCountMinSketch(n)\n",
    "countIpOnDomain(\"124.81.124.112\",\"wikipedia.org\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many unique IPs are there for each domain?\n",
    "\n",
    "Here we need to find the unique ips in each domain. Here the idea is to impliment multiple HLL for each domain.\n",
    "Thi choice is made because memory used by each HLL is very less it is feasible to make a dictionary with keys to be domains and value as HLL buckets. Also the parameters for HLL is chosen by allowing to have an error of 4 to 5% which implies from the error rate equation $\\frac{1.04}{\\sqrt{2^b}}$ the b value to be around 10.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of unique estimated ips in domain wikipedia.org are 50522.5393713267\n",
      "No. of True ips in domain wikipedia.org is 52353\n",
      "error is 3.4963815419809814\n",
      "No of unique estimated ips in domain pandas.pydata.org are 12850.462560135693\n",
      "No. of True ips in domain pandas.pydata.org is 12999\n",
      "error is 1.142683590001593\n",
      "No of unique estimated ips in domain python.org are 25610.173833989553\n",
      "No. of True ips in domain python.org is 25927\n",
      "error is 1.2219931577523322\n",
      "No of unique estimated ips in domain spark.apache.org are 522.0859266999656\n",
      "No. of True ips in domain spark.apache.org is 520\n",
      "error is 0.4011397499933895\n",
      "No of unique estimated ips in domain google.com are 2560.6707269182866\n",
      "No. of True ips in domain google.com is 2626\n",
      "error is 2.487786484452147\n",
      "No of unique estimated ips in domain dtu.dk are 2888.292292020369\n",
      "No. of True ips in domain dtu.dk is 2629\n",
      "error is 9.862772613935684\n",
      "No of unique estimated ips in domain github.com are 1346.253524923271\n",
      "No. of True ips in domain github.com is 1341\n",
      "error is 0.39176173924466934\n",
      "No of unique estimated ips in domain databricks.com are 1431.6362924768591\n",
      "No. of True ips in domain databricks.com is 1368\n",
      "error is 4.651775765852275\n",
      "No of unique estimated ips in domain datarobot.com are 236.27847275688194\n",
      "No. of True ips in domain datarobot.com is 234\n",
      "error is 0.973706306359803\n"
     ]
    }
   ],
   "source": [
    "\n",
    "web_traffic_stream = stream(STREAM_SIZE)\n",
    "b=10\n",
    "domain_unique_ip(bucket_dictionary,web_traffic_stream,b)\n",
    "STREAM_SIZE = 100000\n",
    "web_traffic_stream = stream(STREAM_SIZE)\n",
    "true_values = {}\n",
    "for i,l in enumerate(web_traffic_stream):\n",
    "    if l.split()[1] in true_values.keys():\n",
    "        true_values[l.split()[1]].append(l.split()[0])\n",
    "    else:\n",
    "        true_values[l.split()[1]] = [l.split()[0]]\n",
    "for l in bucket_dictionary.keys():\n",
    "    estimated_ip = get_cardinality(bucket_dictionary[l],b,0.7213/(1+1.079/(2**b)))\n",
    "    true_ip = len(list(set(true_values[l])))\n",
    "    print(\"No of unique estimated ips in domain {} are {}\".format(l,estimated_ip))\n",
    "    print(\"No. of True ips in domain {} is {}\".format(l,true_ip))\n",
    "    print(\"error is {}\".format(100*abs((true_ip-estimated_ip)/(true_ip))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we could see from above that  4 to 5% is also justified by comparing with true values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most X frequent IP's in stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'128.76.240.132': 75,\n",
       " '151.112.165.145': 75,\n",
       " '158.159.192.137': 75,\n",
       " '44.98.181.83': 75,\n",
       " '177.158.218.97': 75,\n",
       " '154.113.149.55': 74,\n",
       " '183.105.132.76': 74,\n",
       " '121.81.207.182': 74,\n",
       " '164.65.148.120': 75,\n",
       " '75.220.155.79': 76,\n",
       " '101.136.92.86': 74,\n",
       " '87.169.85.108': 74,\n",
       " '174.117.171.123': 78,\n",
       " '161.166.109.171': 74,\n",
       " '153.147.118.113': 75,\n",
       " '164.138.98.124': 75,\n",
       " '186.197.114.139': 75,\n",
       " '113.58.134.64': 74,\n",
       " '136.108.134.135': 75,\n",
       " '49.50.50.159': 74,\n",
       " '137.170.154.83': 74,\n",
       " '163.70.109.155': 77,\n",
       " '204.28.120.155': 75,\n",
       " '168.120.130.122': 74,\n",
       " '119.108.102.158': 75,\n",
       " '0.138.125.209': 76,\n",
       " '82.69.113.168': 75,\n",
       " '70.168.130.156': 76,\n",
       " '28.156.118.159': 74,\n",
       " '269.64.127.77': 74,\n",
       " '92.175.129.212': 75,\n",
       " '158.110.147.43': 75,\n",
       " '227.112.50.155': 75,\n",
       " '98.180.195.200': 76,\n",
       " '113.170.90.76': 74,\n",
       " '113.85.116.139': 75,\n",
       " '52.82.137.64': 74,\n",
       " '148.159.194.191': 74,\n",
       " '108.52.137.92': 74,\n",
       " '117.114.112.48': 74,\n",
       " '103.201.106.76': 79,\n",
       " '131.139.255.102': 74,\n",
       " '140.52.237.136': 74,\n",
       " '165.224.177.93': 77,\n",
       " '74.144.187.156': 77,\n",
       " '95.100.167.231': 77,\n",
       " '117.152.-22.164': 75,\n",
       " '113.56.136.150': 76,\n",
       " '130.166.219.202': 75,\n",
       " '100.76.82.161': 75}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateCountMinSketch_heap(10000,50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
